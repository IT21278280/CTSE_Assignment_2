{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ae8ad4",
   "metadata": {},
   "source": [
    "# CTSE Lecture Notes Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7761c7e",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582f54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the CTSE Lecture Notes Chatbot\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import PyPDF2\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d474b",
   "metadata": {},
   "source": [
    "Set Up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2837543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for debugging and monitoring\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Specify Tesseract path (Windows example; adjust for your system)\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Uncomment and adjust if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57d629",
   "metadata": {},
   "source": [
    "Define Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4dd3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function to remove artifacts and improve formatting\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text.strip())\n",
    "    text = re.sub(r'●|\\○|\\-|\\+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065659d5",
   "metadata": {},
   "source": [
    "Load and Process Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a443d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process lecture notes with page tracking and OCR fallback\n",
    "def load_and_process_documents():\n",
    "    logging.info(\"Loading and processing lecture notes...\")\n",
    "    notes_dir = './lecture_notes/'\n",
    "    if not os.path.exists(notes_dir):\n",
    "        logging.error(f\"Directory {notes_dir} not found.\")\n",
    "        print(f\"Directory {notes_dir} not found. Please create it and add PDF files.\")\n",
    "        return None\n",
    "\n",
    "    documents = []\n",
    "    for pdf_file in os.listdir(notes_dir):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(notes_dir, pdf_file)\n",
    "            try:\n",
    "                with open(pdf_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    text_extracted = False\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        text = page.extract_text()\n",
    "                        if text and text.strip():\n",
    "                            cleaned_text = clean_text(text)\n",
    "                            doc = Document(page_content=cleaned_text, metadata={\"source\": pdf_file, \"page\": page_num + 1})\n",
    "                            documents.append(doc)\n",
    "                            text_extracted = True\n",
    "                        else:\n",
    "                            logging.warning(f\"No text extracted from {pdf_file}, page {page_num + 1}. Falling back to OCR.\")\n",
    "                            break\n",
    "\n",
    "                if not text_extracted:\n",
    "                    logging.info(f\"Using OCR for {pdf_file}...\")\n",
    "                    images = convert_from_path(pdf_path)\n",
    "                    for page_num, image in enumerate(images):\n",
    "                        text = pytesseract.image_to_string(image)\n",
    "                        if text and text.strip():\n",
    "                            cleaned_text = clean_text(text)\n",
    "                            doc = Document(page_content=cleaned_text, metadata={\"source\": pdf_file, \"page\": page_num + 1})\n",
    "                            documents.append(doc)\n",
    "                        else:\n",
    "                            logging.warning(f\"OCR failed to extract text from {pdf_file}, page {page_num + 1}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {pdf_file}: {e}\")\n",
    "                print(f\"Error processing {pdf_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not documents:\n",
    "        logging.error(\"No documents loaded after processing.\")\n",
    "        print(\"No documents loaded. Ensure PDFs are text-based or OCR-compatible.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    logging.info(f\"Created {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d46b5",
   "metadata": {},
   "source": [
    "Set Up Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ddf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vector store with FAISS\n",
    "def setup_vector_store(chunks):\n",
    "    logging.info(\"Setting up vector store...\")\n",
    "    faiss_index_path = './faiss_index'\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        vector_store = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_store.save_local(faiss_index_path)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6ce16",
   "metadata": {},
   "source": [
    "Set Up Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama LLM with Gemma:2b\n",
    "def setup_llm():\n",
    "    logging.info(\"Initializing Ollama LLM with Gemma:2b...\")\n",
    "    try:\n",
    "        return OllamaLLM(model=\"gemma:2b\", temperature=0.3)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing Ollama: {e}\")\n",
    "        print(f\"Error initializing Ollama: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c23ce2",
   "metadata": {},
   "source": [
    "Build RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f5a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 00:10:46,743 - INFO - Loading and processing lecture notes...\n",
      "2025-05-12 00:10:49,968 - WARNING - No text extracted from lecture4.pdf, page 13. Falling back to OCR.\n",
      "2025-05-12 00:10:49,972 - INFO - Created 163 chunks.\n",
      "2025-05-12 00:10:49,973 - INFO - Setting up vector store...\n",
      "2025-05-12 00:10:49,975 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-12 00:10:49,976 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-05-12 00:10:53,132 - INFO - Initializing Ollama LLM with Gemma:2b...\n",
      "2025-05-12 00:10:53,693 - INFO - Building RAG pipeline...\n"
     ]
    }
   ],
   "source": [
    "# Build Retrieval-Augmented Generation (RAG) pipeline\n",
    "def setup_qa_chain(vector_store, llm):\n",
    "    logging.info(\"Building RAG pipeline...\")\n",
    "    prompt_template = \"\"\"\n",
    "    You are a specialized assistant for Current Trends in Software Engineering (CTSE) lecture notes. Your task is to provide a clear, concise, and accurate answer using *only* the exact text from the provided CTSE lecture notes. Copy the relevant sentence(s) or phrase(s) directly from the context without adding, modifying, or elaborating on the content. Use bullet points or short sentences for clarity. If the question is unrelated to CTSE lecture notes (e.g., sports, general knowledge), respond exactly with: 'This question is outside the scope of CTSE lecture notes. Please ask about topics from the lecture notes.' If no relevant information is found, respond exactly with: 'No relevant information found in the CTSE lecture notes.'\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "# Load data and set up the chatbot\n",
    "chunks = load_and_process_documents()\n",
    "if chunks:\n",
    "    vector_store = setup_vector_store(chunks)\n",
    "    llm = setup_llm()\n",
    "    if vector_store and llm:\n",
    "        qa_chain = setup_qa_chain(vector_store, llm)\n",
    "    else:\n",
    "        print(\"Failed to set up chatbot. Check logs.\")\n",
    "        raise SystemExit\n",
    "else:\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf05c",
   "metadata": {},
   "source": [
    "Implement Chatbot Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de220713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ask questions with improved retrieval and concise output\n",
    "def ask_question(question):\n",
    "    if qa_chain:\n",
    "        try:\n",
    "            # Extract lecture name and page range from the question (if applicable)\n",
    "            lecture_match = re.search(r'Lecture(\\d+)', question, re.IGNORECASE)\n",
    "            page_range = None\n",
    "            if \"first 2 page\" in question.lower() or \"page 1 to 2\" in question.lower():\n",
    "                page_range = [1, 2]\n",
    "\n",
    "            if lecture_match and page_range:\n",
    "                lecture_num = lecture_match.group(1)\n",
    "                lecture_file = f\"lecture{lecture_num}.pdf\"\n",
    "                filtered_docs = [doc for doc in chunks if doc.metadata[\"source\"] == lecture_file and doc.metadata[\"page\"] in page_range]\n",
    "                if not filtered_docs:\n",
    "                    return \"No content found for the first two pages of Lecture {}.\".format(lecture_num), \"\"\n",
    "                temp_vector_store = FAISS.from_documents(filtered_docs, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "                temp_qa_chain = RetrievalQA.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    chain_type=\"stuff\",\n",
    "                    retriever=temp_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                    return_source_documents=True,\n",
    "                    chain_type_kwargs={\"prompt\": qa_chain.combine_documents_chain.llm_chain.prompt}\n",
    "                )\n",
    "                result = temp_qa_chain.invoke({\"query\": question})\n",
    "            else:\n",
    "                # For general questions, use the main vector store with a refined keyword filter\n",
    "                keywords = [word for word in question.lower().split() if word not in ['what', 'is', 'in', 'the', 'a', 'an']]\n",
    "                filtered_docs = [\n",
    "                    doc for doc in chunks\n",
    "                    if any(keyword in doc.page_content.lower() for keyword in keywords)\n",
    "                ]\n",
    "                if not filtered_docs:\n",
    "                    return \"No relevant information found in the CTSE lecture notes.\", \"\"\n",
    "                temp_vector_store = FAISS.from_documents(filtered_docs, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "                temp_qa_chain = RetrievalQA.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    chain_type=\"stuff\",\n",
    "                    retriever=temp_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                    return_source_documents=True,\n",
    "                    chain_type_kwargs={\"prompt\": qa_chain.combine_documents_chain.llm_chain.prompt}\n",
    "                )\n",
    "                result = temp_qa_chain.invoke({\"query\": question})\n",
    "\n",
    "            answer = result[\"result\"].strip()\n",
    "            answer = clean_text(answer)  # Ensure clean and concise output\n",
    "            source = result[\"source_documents\"][0].page_content[:200] + \"...\" if result[\"source_documents\"] else \"No source available.\"\n",
    "            source = clean_text(source)\n",
    "            logging.info(f\"Question: {question}\")\n",
    "            logging.info(f\"Answer: {answer}\")\n",
    "            return answer, source\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing question: {e}\")\n",
    "            return f\"Error: Unable to process question. {e}\", \"\"\n",
    "    return \"Chatbot not initialized.\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf01fb8",
   "metadata": {},
   "source": [
    "Test Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac37933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTSE Lecture Notes Chatbot ✨ (Gemma:2b + LangChain)\n",
      "Type 'exit' to stop the chatbot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 02:13:12,490 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-12 02:13:12,496 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-05-12 02:13:45,547 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 02:13:55,173 - INFO - Question: What is zero padding in boundary handling?\n",
      "2025-05-12 02:13:55,175 - INFO - Answer: Sure, here's the answer to your question: **zero padding in boundary handling refers to the process of filling in the missing pixels with zeros when processing pixels near the edges of an image.**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You: What is zero padding in boundary handling?\n",
      "Bot: Sure, here's the answer to your question: **zero padding in boundary handling refers to the process of filling in the missing pixels with zeros when processing pixels near the edges of an image.**\n",
      "Source: . boundary issues: when processing pixels near the edges of an image, special handling is needed because some neighboring pixels might be outside the image boundary. mask math: the new pixel value is ...\n",
      "--------------------------------------------------\n",
      "\n",
      "Chat History:\n",
      "You: What is zero padding in boundary handling?\n",
      "Bot: Sure, here's the answer to your question: **zero padding in boundary handling refers to the process of filling in the missing pixels with zeros when processing pixels near the edges of an image.**\n",
      "Source: . boundary issues: when processing pixels near the edges of an image, special handling is needed because some neighboring pixels might be outside the image boundary. mask math: the new pixel value is ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot in Jupyter Notebook\n",
    "chat_history = []\n",
    "\n",
    "print(\"CTSE Lecture Notes Chatbot ✨ (Gemma:2b + LangChain)\")\n",
    "print(\"Type 'exit' to stop the chatbot.\")\n",
    "while True:\n",
    "    question = input(\"Ask a question about CTSE Software Engineering: \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    if question:\n",
    "        answer, source = ask_question(question)\n",
    "        chat_history.append({\"question\": question, \"answer\": answer, \"source\": source})\n",
    "        print(\"\\nYou:\", question)\n",
    "        print(\"Bot:\", answer)\n",
    "        print(\"Source:\", source)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Display chat history\n",
    "print(\"\\nChat History:\")\n",
    "for chat in chat_history:\n",
    "    print(\"You:\", chat[\"question\"])\n",
    "    print(\"Bot:\", chat[\"answer\"])\n",
    "    print(\"Source:\", chat[\"source\"])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
